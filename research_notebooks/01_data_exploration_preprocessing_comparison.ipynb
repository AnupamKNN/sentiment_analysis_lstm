{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e4935f8",
   "metadata": {},
   "source": [
    "# **Social Media Sentiment & Trend Analysis Platform**\n",
    "\n",
    "<div style=\"background-color: #000000ff; padding: 20px; border-left: 5px solid #007bff; margin: 20px 0;\">\n",
    "<h3>üè¢ <strong>Client:</strong> VelociSense Analytics</h3>\n",
    "<h3>üìä <strong>Project:</strong> Enterprise Social Media Intelligence Platform</h3>\n",
    "<h3>üë®‚Äçüíº <strong>Data Scientist:</strong> Anupam Anand Singh</h3>\n",
    "<h3>üìÖ <strong>Project Timeline:</strong> October 2025</h3>\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "## **üéØ Executive Summary**\n",
    "\n",
    "VelociSense Analytics has been commissioned to develop an **enterprise-grade social media sentiment analysis platform** that provides real-time insights into brand perception, customer sentiment trends, and predictive analytics for business decision-making. This project demonstrates a comprehensive approach to NLP model development, from systematic technique comparison to production-ready MLOps deployment.\n",
    "\n",
    "***\n",
    "\n",
    "## **üë• Project Stakeholders**\n",
    "\n",
    "| **Role** | **Name** | **Responsibility** |\n",
    "|----------|----------|-------------------|\n",
    "| **Chief Marketing Officer (Executive Sponsor)** | Rajesh Nambiar | Strategic oversight, budget approval, ROI validation, business case alignment |\n",
    "| **Head of Social Media Analytics (Primary Stakeholder)** | Priya Sharma | Domain expertise, sentiment analysis requirements, business KPI definition |\n",
    "| **Director of Customer Experience (Key Stakeholder)** | Amit Khanna | Customer journey insights, brand reputation impact assessment, crisis response protocols |\n",
    "| **Project Manager** | Kavya Desai | Timeline management, cross-functional coordination, milestone tracking, stakeholder communication |\n",
    "| **Lead Data Scientist** | **Anupam Singh** | NLP model development, systematic comparison methodology, feature engineering, model selection |\n",
    "| **MLOps Engineer** | Ravi Patel | CI/CD pipeline setup, Docker containerization, Apache Airflow orchestration, model deployment |\n",
    "| **Data Engineer** | Sneha Gupta | Data ingestion from social platforms, ETL pipeline development, data quality assurance |\n",
    "| **Backend Developer** | Karthik Reddy | FastAPI development, database integration, API endpoint creation, system scalability |\n",
    "| **Frontend Developer** | Neha Agarwal | Streamlit dashboard development, data visualization, user interface design, executive reporting |\n",
    "| **DevOps Engineer** | Vikram Singh | Cloud infrastructure setup, monitoring systems, deployment automation, performance optimization |\n",
    "| **QA Engineer** | Pooja Jain | Testing framework implementation, model validation, integration testing, performance benchmarking |\n",
    "| **Compliance Manager (Stakeholder)** | Aruna Krishnan | Data privacy compliance, GDPR adherence, social media data usage policies, audit requirements |\n",
    "| **Business Analyst** | Rohit Malhotra | Requirements gathering, use case validation, business metrics definition, ROI analysis |\n",
    "| **IT Security Lead (Consultant)** | Suresh Kumar | Security protocols, data encryption, API security, vulnerability assessment |\n",
    "\n",
    "***\n",
    "\n",
    "## **üìã Stakeholder Engagement Matrix**\n",
    "\n",
    "| **Stakeholder Category** | **Engagement Level** | **Communication Frequency** | **Key Deliverables** |\n",
    "|-------------------------|---------------------|---------------------------|----------------------|\n",
    "| **Executive Sponsors** | Strategic Decision Making | Weekly Status Reports | Business Impact Analysis, ROI Projections |\n",
    "| **Primary Stakeholders** | Active Collaboration | Daily/Bi-daily Updates | Technical Requirements, Model Performance Metrics |\n",
    "| **Technical Team** | Hands-on Implementation | Daily Standups | Code Reviews, Technical Documentation |\n",
    "| **Support Functions** | Consultative | Weekly Check-ins | Compliance Reports, Security Assessments |\n",
    "\n",
    "***\n",
    "\n",
    "## **üéØ Stakeholder Success Criteria**\n",
    "\n",
    "### **Business Stakeholders**\n",
    "- **CMO (Rajesh Nambiar)**: 25% improvement in customer sentiment response time, $2.3M cost savings annually\n",
    "- **Head of Social Analytics (Priya Sharma)**: >90% model accuracy, real-time dashboard functionality\n",
    "- **Director of CX (Amit Khanna)**: 30% faster crisis detection, automated escalation workflows\n",
    "\n",
    "### **Technical Stakeholders**\n",
    "- **Lead Data Scientist ([Your Name])**: Scientific model comparison methodology, production-ready ML pipeline\n",
    "- **MLOps Engineer (Ravi Patel)**: 99.9% system uptime, <200ms API response time\n",
    "- **Data Engineer (Sneha Gupta)**: 1M+ posts processed daily, zero data quality issues\n",
    "\n",
    "### **Support Stakeholders**\n",
    "- **Compliance Manager (Aruna Krishnan)**: 100% regulatory compliance, audit-ready documentation\n",
    "- **QA Engineer (Pooja Jain)**: Comprehensive test coverage, performance benchmarks achieved\n",
    "\n",
    "***\n",
    "\n",
    "## **üè¢ Business Context & Problem Statement**\n",
    "\n",
    "### **Client Profile: VelociSense Analytics**\n",
    "VelociSense Analytics is a leading **AI-powered social media intelligence company** serving enterprise clients across tourism, retail, hospitality, and financial services sectors. The company specializes in transforming unstructured social media data into actionable business insights through advanced NLP and machine learning technologies.[2][3]\n",
    "\n",
    "### **Market Challenge**\n",
    "In today's digital landscape, businesses face overwhelming volumes of social media data with **limited ability to extract actionable insights**:\n",
    "\n",
    "- **Scale Challenge**: Processing millions of social media posts, reviews, and comments daily\n",
    "- **Speed Requirement**: Real-time sentiment detection for crisis management and opportunity identification  \n",
    "- **Accuracy Demand**: High-precision sentiment classification for business-critical decisions\n",
    "- **Context Complexity**: Understanding nuanced sentiment in diverse linguistic expressions, sarcasm, and cultural contexts\n",
    "\n",
    "### **Business Impact**\n",
    "Poor sentiment analysis capabilities result in:\n",
    "- **Revenue Loss**: Missed opportunities from positive sentiment trends (estimated $2.3M annually for mid-size retail clients)\n",
    "- **Brand Damage**: Delayed response to negative sentiment crises (average 23% reputation score drop)\n",
    "- **Competitive Disadvantage**: Inability to track competitor sentiment benchmarks effectively\n",
    "- **Operational Inefficiency**: Manual sentiment analysis consuming 40+ hours weekly per analyst\n",
    "\n",
    "***\n",
    "\n",
    "## **üéØ Project Objectives & Success Metrics**\n",
    "\n",
    "### **Primary Objectives**\n",
    "1. **Develop Superior NLP Pipeline**: Create a comprehensive sentiment analysis system outperforming existing solutions by 15% in F1-score\n",
    "2. **Systematic Model Comparison**: Implement scientific methodology for comparing all major NLP techniques and architectures\n",
    "3. **Production-Ready Deployment**: Build scalable MLOps infrastructure supporting 100K+ predictions per hour\n",
    "4. **Business Value Demonstration**: Generate actionable insights leading to measurable ROI for client use cases\n",
    "\n",
    "### **Success Metrics**\n",
    "- **Technical Performance**: >90% F1-score on Sentiment140 benchmark dataset\n",
    "- **Processing Speed**: <200ms average inference time per document\n",
    "- **Scalability**: Handle 1M+ social media posts daily with 99.9% uptime\n",
    "- **Business Impact**: Enable 30% faster crisis response times and 25% improvement in customer satisfaction scores\n",
    "\n",
    "***\n",
    "\n",
    "## **üíº Use Cases & Applications**\n",
    "\n",
    "### **Tourism Industry Applications**\n",
    "- **Destination Reputation Monitoring**: Real-time sentiment tracking for tourist destinations across review platforms\n",
    "- **Crisis Management**: Automated alert systems for negative publicity events (natural disasters, safety concerns)\n",
    "- **Seasonal Trend Prediction**: Sentiment-based forecasting for tourism demand planning\n",
    "- **Competitive Intelligence**: Benchmark sentiment performance against competitor destinations\n",
    "\n",
    "### **Retail Sector Applications**\n",
    "- **Product Launch Analysis**: Sentiment tracking for new product introductions and feature releases\n",
    "- **Influencer Campaign ROI**: Measure sentiment impact of influencer partnerships and collaborations\n",
    "- **Customer Service Optimization**: Automated routing of negative sentiment cases to priority support queues\n",
    "- **Inventory Planning**: Sentiment-driven demand forecasting for seasonal merchandise\n",
    "\n",
    "### **Financial Services Applications**\n",
    "- **Brand Risk Assessment**: Early warning systems for reputation risks affecting stock performance\n",
    "- **Customer Retention**: Identify at-risk customers through negative sentiment patterns\n",
    "- **Product Development Insights**: Sentiment analysis of financial product reviews and feedback\n",
    "- **Regulatory Compliance**: Monitor public sentiment regarding regulatory changes and compliance issues\n",
    "\n",
    "***\n",
    "\n",
    "## **üî¨ Technical Approach & Methodology**\n",
    "\n",
    "### **Phase 1: Comprehensive NLP Technique Evaluation**\n",
    "This project implements a **systematic comparison framework** evaluating every major NLP approach to ensure optimal model selection based on empirical evidence rather than assumptions.\n",
    "\n",
    "#### **Text Preprocessing Comparison Matrix**\n",
    "- **Cleaning Techniques**: HTML/XML removal, URL extraction, emoji processing, special character handling\n",
    "- **Normalization Methods**: Stemming (Porter, Snowball, Lancaster) vs. Lemmatization (WordNet, spaCy)\n",
    "- **Tokenization Strategies**: Word-level, subword-level, character-level, domain-specific tokenizers\n",
    "- **Stopword Strategies**: Standard removal, custom domain stopwords, no removal baseline\n",
    "\n",
    "#### **Feature Engineering Evaluation**\n",
    "- **Classical Methods**: Bag of Words, TF-IDF (uni/bi/tri-grams), N-gram analysis\n",
    "- **Modern Embeddings**: Word2Vec, GloVe, FastText (pre-trained vs. custom-trained)\n",
    "- **Advanced Techniques**: Doc2Vec, Sentence embeddings, contextual embeddings\n",
    "\n",
    "#### **Model Architecture Comparison**\n",
    "- **Deep Learning**: MLP baseline, RNN variants, LSTM (uni/bidirectional), GRU (uni/bidirectional), Hybrid LSTM-GRU\n",
    "- **Advanced Architectures**: Attention mechanisms, Transformer components (encoder-only)\n",
    "\n",
    "### **Phase 2: Production Pipeline Development**\n",
    "Upon identifying the optimal approach through systematic comparison, implement **enterprise-grade MLOps pipeline**:\n",
    "\n",
    "- **Data Ingestion**: Apache Airflow orchestration for real-time social media data collection\n",
    "- **Model Serving**: FastAPI endpoints with auto-scaling capabilities  \n",
    "- **Monitoring**: Real-time performance tracking with MLflow and custom business metrics\n",
    "- **Deployment**: Docker containerization with CI/CD automation via GitHub Actions\n",
    "\n",
    "***\n",
    "\n",
    "## **üìä Expected Deliverables & Timeline**\n",
    "\n",
    "### **Technical Deliverables**\n",
    "1. **Jupyter Notebook Analysis Series** (Weeks 1-3):\n",
    "   - Comprehensive preprocessing technique comparison\n",
    "   - Feature engineering methodology evaluation  \n",
    "   - Model architecture performance analysis\n",
    "   - Statistical significance testing and selection methodology\n",
    "\n",
    "2. **Production Codebase** (Weeks 4-6):\n",
    "   - Modular MLOps pipeline implementation\n",
    "   - API development with Swagger documentation\n",
    "   - Containerized deployment infrastructure\n",
    "   - Comprehensive testing suite (unit, integration, performance)\n",
    "\n",
    "3. **Business Intelligence Dashboard** (Week 7):\n",
    "   - Streamlit-based executive dashboard\n",
    "   - Real-time sentiment monitoring interface\n",
    "   - Automated report generation with GenAI insights\n",
    "   - Performance metrics and ROI tracking\n",
    "\n",
    "### **Business Deliverables**\n",
    "1. **Technical Performance Report**: Detailed comparison of all evaluated approaches with statistical validation\n",
    "2. **Business Impact Analysis**: ROI projections and use case implementation recommendations  \n",
    "3. **Deployment Documentation**: Complete technical specification for production deployment\n",
    "4. **User Training Materials**: Comprehensive guides for business users and technical stakeholders\n",
    "\n",
    "***\n",
    "\n",
    "## **üöÄ Innovation & Competitive Advantage**\n",
    "\n",
    "This project differentiates itself through:\n",
    "\n",
    "- **Scientific Rigor**: Systematic evaluation methodology ensuring optimal model selection\n",
    "- **Production Readiness**: Enterprise-grade MLOps implementation from day one\n",
    "- **Business Focus**: Real-world use case validation with measurable ROI metrics\n",
    "- **Scalability Design**: Architecture supporting millions of daily predictions with sub-second latency\n",
    "- **GenAI Integration**: Advanced report generation using LLM technology for executive insights\n",
    "\n",
    "***\n",
    "\n",
    "<div style=\"background-color: #000000ff; padding: 15px; border-radius: 8px; margin: 20px 0;\">\n",
    "<h3>üéØ <strong>Project Vision</strong></h3>\n",
    "<p><em>\"Transform unstructured social media noise into structured business intelligence through scientifically-validated NLP methodologies and production-ready MLOps infrastructure.\"</em></p>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c067e",
   "metadata": {},
   "source": [
    "### **üìä Dataset Overview: Sentiment140**\n",
    "\n",
    "<div style=\"background-color: #000000ff; padding: 20px; border-left: 5px solid #4CAF50; margin: 20px 0;\">\n",
    "<h3>üéØ <strong>Dataset:</strong> Sentiment140 - Twitter Sentiment Classification</h3>\n",
    "<h3>üìà <strong>Scale:</strong> 1.6 Million Annotated Tweets</h3>\n",
    "<h3>üî¨ <strong>Source:</strong> Stanford University Research</h3>\n",
    "<h3>üìÖ <strong>Collection Period:</strong> 2009 Twitter API Extraction</h3>\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "## **üéØ Dataset Context**\n",
    "\n",
    "The **Sentiment140 dataset** represents one of the largest publicly available sentiment analysis datasets, containing **1,600,000 tweets** extracted using the Twitter API. This dataset has been automatically annotated using distant supervision, making it ideal for training robust sentiment classification models at scale.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Volume**: 1.6M tweets providing substantial training data\n",
    "- **Annotation Method**: Distant supervision using emoticons as noisy labels\n",
    "- **Multi-class Structure**: Negative, Neutral, and Positive sentiment categories\n",
    "- **Real-world Data**: Authentic social media content with natural language variations\n",
    "- **Research Foundation**: Extensively used in academic and industrial NLP research\n",
    "\n",
    "---\n",
    "\n",
    "## **üìã Dataset Schema & Structure**\n",
    "\n",
    "### **Data Fields Overview**\n",
    "\n",
    "| **Field** | **Data Type** | **Description** | **Example Value** |\n",
    "|-----------|---------------|-----------------|-------------------|\n",
    "| `target` | **Integer** | Sentiment polarity of the tweet | `0` = Negative, `2` = Neutral, `4` = Positive |\n",
    "| `ids` | **Integer** | Unique tweet identifier | `2087` |\n",
    "| `date` | **String** | Tweet timestamp | `\"Sat May 16 23:58:44 UTC 2009\"` |\n",
    "| `flag` | **String** | Query flag for tweet collection | `\"lyx\"` or `\"NO_QUERY\"` |\n",
    "| `user` | **String** | Twitter username (anonymized) | `\"robotickilldozr\"` |\n",
    "| `text` | **String** | Raw tweet content | `\"Lyx is cool\"` |\n",
    "\n",
    "### **Target Variable Distribution**\n",
    "\n",
    "The sentiment labels follow a specific encoding scheme:\n",
    "- **Negative (0)**: Approximately 800,000 tweets expressing negative sentiment  \n",
    "- **Neutral (2)**: Limited/No samples available in the dataset\n",
    "- **Positive (4)**: Approximately 800,000 tweets expressing positive sentiment\n",
    "\n",
    "### **Important Dataset Characteristics**\n",
    "\n",
    "**Primary Classification Pattern:**\n",
    "While the original schema supports three sentiment categories, the Sentiment140 dataset predominantly contains **binary classifications** with minimal neutral samples. This creates an effectively balanced binary sentiment dataset with equal representation of positive and negative sentiments.\n",
    "\n",
    "**Practical Distribution:**\n",
    "- **Negative Sentiment**: ~50% of total dataset\n",
    "- **Positive Sentiment**: ~50% of total dataset  \n",
    "- **Neutral Sentiment**: Minimal presence in practice\n",
    "\n",
    "***\n",
    "\n",
    "## **üîç Data Quality Characteristics**\n",
    "\n",
    "### **Preprocessing Status**\n",
    "- **Emoticons**: Removed from tweet text after being used for distant supervision labeling\n",
    "- **Raw Format**: CSV with minimal preprocessing, preserving natural language patterns\n",
    "- **Encoding**: UTF-8 compatible with special character handling requirements\n",
    "- **Missing Values**: Minimal null values across all fields\n",
    "\n",
    "### **Text Content Analysis**\n",
    "- **Language**: Primarily English tweets with occasional multilingual content\n",
    "- **Length Variation**: Variable tweet lengths up to 140 characters (2009 Twitter limit)\n",
    "- **Content Diversity**: Mixed topics, informal language, slang, abbreviations, hashtags\n",
    "- **Noise Factors**: Typos, mentions, URLs, special characters requiring systematic preprocessing\n",
    "\n",
    "### **Temporal Considerations**\n",
    "- **Collection Period**: May 2009 Twitter data\n",
    "- **Language Evolution**: May not reflect current social media language patterns\n",
    "- **Platform Changes**: Pre-dates modern Twitter features and character limits\n",
    "\n",
    "***\n",
    "\n",
    "## **üìä Dataset Splits & Availability**\n",
    "\n",
    "### **Standard Data Configuration**\n",
    "| **Component** | **Size** | **Purpose** | **Usage in VelociSense Analytics** |\n",
    "|---------------|----------|-------------|-------------------------------------|\n",
    "| **Training Data** | 1,600,000 tweets | Model development | Systematic preprocessing & feature comparison |\n",
    "| **Test Data** | 498 tweets | Final evaluation | Independent performance validation |\n",
    "***\n",
    "\n",
    "## **üéØ Research Foundation & Academic Validation**\n",
    "\n",
    "### **Original Research Paper**\n",
    "**Title**: \"Twitter Sentiment Classification using Distant Supervision\"  \n",
    "**Authors**: Go, Alec and Bhayani, Richa and Huang, Lei  \n",
    "**Institution**: Stanford University Computer Science Department  \n",
    "**Publication Year**: 2009  \n",
    "**Research Impact**: Pioneered distant supervision approach for large-scale sentiment analysis\n",
    "\n",
    "### **Methodological Innovation**\n",
    "The dataset introduced the concept of **distant supervision** for sentiment analysis, using emoticons as automatic labels:\n",
    "- **Happy emoticons** ‚Üí Positive labels\n",
    "- **Sad emoticons** ‚Üí Negative labels\n",
    "- **Emoticon removal** ‚Üí Clean text for training\n",
    "\n",
    "---\n",
    "\n",
    "## **üöÄ Strategic Advantages for VelociSense Analytics**\n",
    "\n",
    "### **Scale & Performance Benefits**\n",
    "- **Large-scale Training**: 1.6M samples enable robust deep learning model development\n",
    "- **Balanced Distribution**: Equal positive/negative representation prevents model bias\n",
    "- **Real-world Complexity**: Authentic social media language tests preprocessing robustness\n",
    "- **Benchmark Compatibility**: Enables comparison with established research results\n",
    "\n",
    "### **Business Application Relevance**\n",
    "- **Social Media Focus**: Direct applicability to Twitter and similar platform analysis\n",
    "- **Scalability Validation**: Large dataset tests production system performance\n",
    "- **Noise Handling**: Real-world data validates preprocessing technique effectiveness\n",
    "- **Binary Decision Making**: Aligns with business requirements for clear positive/negative classification\n",
    "\n",
    "### **Technical Development Support**\n",
    "- **Methodology Comparison**: Sufficient data for systematic NLP technique evaluation\n",
    "- **Statistical Validation**: Large sample size enables robust performance comparisons\n",
    "- **Production Testing**: Scale appropriate for enterprise deployment validation\n",
    "\n",
    "***\n",
    "\n",
    "<div style=\"background-color: #000000ff; padding: 15px; border-radius: 8px; margin: 20px 0;\">\n",
    "<h3>‚ö†Ô∏è <strong>Dataset Considerations</strong></h3>\n",
    "<ul>\n",
    "<li><strong>Temporal Limitation</strong>: 2009 data may not reflect current social media language evolution</li>\n",
    "<li><strong>Distant Supervision Noise</strong>: Emoticon-based labeling may introduce annotation inconsistencies</li>\n",
    "<li><strong>Binary Focus</strong>: Limited neutral sentiment data reduces three-class classification options</li>\n",
    "<li><strong>Language Scope</strong>: Primarily English content with limited multilingual representation</li>\n",
    "<li><strong>Platform Specificity</strong>: Twitter-specific format may require adaptation for other platforms</li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de11b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "def download_kaggle_dataset(dataset_slug, destination_dir):\n",
    "    # Download dataset to the default location\n",
    "    path = kagglehub.dataset_download(dataset_slug)\n",
    "    \n",
    "    # Ensure the destination directory exists\n",
    "    os.makedirs(destination_dir, exist_ok=True)\n",
    "    \n",
    "    # Move files to the preferred directory\n",
    "    if os.path.exists(path):\n",
    "        # Get the dataset name from the path\n",
    "        dataset_name = os.path.basename(path)\n",
    "        final_destination = os.path.join(destination_dir, dataset_name)\n",
    "        \n",
    "        # Move to destination\n",
    "        shutil.move(path, final_destination)\n",
    "        print(f\"Dataset moved to: {final_destination}\")\n",
    "        return final_destination\n",
    "    else:\n",
    "        print(\"Download path does not exist.\")\n",
    "        return None\n",
    "\n",
    "# Use the function\n",
    "dataset_path = download_kaggle_dataset(\"kazanova/sentiment140\", \"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280c76eb",
   "metadata": {},
   "source": [
    "### Let us load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440c65bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Shape: (1600000, 6)\n",
      "Columns: ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
      "\n",
      "First few rows:\n",
      "   sentiment          id                          date     query  \\\n",
      "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define column names for the Sentiment140 dataset\n",
    "columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Load the dataset with correct encoding and column names\n",
    "df = pd.read_csv(\n",
    "    \"data/sentiment140.csv\", \n",
    "    encoding='latin-1', \n",
    "    header=None, \n",
    "    names=columns\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236bdda6",
   "metadata": {},
   "source": [
    "### Let us begin conducting exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b0bf3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment          id                          date     query  \\\n",
       "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4585506",
   "metadata": {},
   "source": [
    "### Checking the shape of the dataset is important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5fdf471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a7a11",
   "metadata": {},
   "source": [
    "In the above dataset, we have **1,600,000** rows and **6** columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21f7d86",
   "metadata": {},
   "source": [
    "### Assessing the distribution of target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f141cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " SENTIMENT DISTRIBUTIION ANALYSIS\n",
      "==================================================\n",
      "Sentiment Distribution:\n",
      " Negative (0): 800000 tweets (50.00%)\n",
      " Positive (4): 800000 tweets (50.00%)\n",
      "\n",
      "Class Balance Ratio: 1.0 = 800000 / 800000.000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n SENTIMENT DISTRIBUTIION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Value couynts and percentages\n",
    "sentiment_counts = df['sentiment'].value_counts().sort_index()\n",
    "sentiment_percentages = df['sentiment'].value_counts(normalize = True).sort_index() * 100\n",
    "\n",
    "print(\"Sentiment Distribution:\")\n",
    "for sentiment, count in sentiment_counts.items():\n",
    "    percentage = sentiment_percentages[sentiment]\n",
    "    label = \"Negative\" if sentiment == 0 else \"Neutral\" if sentiment == 2 else \"Positive\"\n",
    "    print(f\" {label} ({sentiment}): {count} tweets ({percentage:.2f}%)\")\n",
    "\n",
    "# Check for class imbalance\n",
    "print(f\"\\nClass Balance Ratio: {sentiment_counts.min() / sentiment_counts.max()} = {sentiment_counts.min()} / {sentiment_counts.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e4b613",
   "metadata": {},
   "source": [
    "As per the above analysis, the dataset is **balanced** with **positive** and **negative** sentiment distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16de94",
   "metadata": {},
   "source": [
    "### Let us now check the data for missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b00f0520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç MISSING VALUE ANALYSIS\n",
      "==================================================\n",
      "           Missing_Count  Missing_Percentage Data_Type\n",
      "sentiment              0                 0.0     int64\n",
      "id                     0                 0.0     int64\n",
      "date                   0                 0.0    object\n",
      "query                  0                 0.0    object\n",
      "user                   0                 0.0    object\n",
      "text                   0                 0.0    object\n",
      "\n",
      "Total rows with missing values: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç MISSING VALUE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100,\n",
    "    'Data_Type': df.dtypes\n",
    "})\n",
    "\n",
    "print(missing_stats)\n",
    "\n",
    "# Identify rows with missing values\n",
    "rows_with_missing = df.isnull().any(axis=1).sum()\n",
    "print(f\"\\nTotal rows with missing values: {rows_with_missing:,} ({rows_with_missing/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5909e8",
   "metadata": {},
   "source": [
    "As per the above analysis, the dataset has **no missing values**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3625d1fc",
   "metadata": {},
   "source": [
    "### Let us now conduct Text Content Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "544c4ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù TEXT CONTENT ANALYSIS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length Statistics:\n",
      "count    1.600000e+06\n",
      "mean     7.409011e+01\n",
      "std      3.644114e+01\n",
      "min      6.000000e+00\n",
      "25%      4.400000e+01\n",
      "50%      6.900000e+01\n",
      "75%      1.040000e+02\n",
      "max      3.740000e+02\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Word Count Statistics:\n",
      "count    1.600000e+06\n",
      "mean     1.317615e+01\n",
      "std      6.957978e+00\n",
      "min      1.000000e+00\n",
      "25%      7.000000e+00\n",
      "50%      1.200000e+01\n",
      "75%      1.900000e+01\n",
      "max      6.400000e+01\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "üìä TEXT METRICS BY SENTIMENT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">text_length</th>\n",
       "      <th colspan=\"4\" halign=\"left\">word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>74.30</td>\n",
       "      <td>36.74</td>\n",
       "      <td>6</td>\n",
       "      <td>359</td>\n",
       "      <td>13.58</td>\n",
       "      <td>7.07</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>73.88</td>\n",
       "      <td>36.14</td>\n",
       "      <td>6</td>\n",
       "      <td>374</td>\n",
       "      <td>12.77</td>\n",
       "      <td>6.82</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text_length                 word_count              \n",
       "                 mean    std min  max       mean   std min max\n",
       "sentiment                                                     \n",
       "0               74.30  36.74   6  359      13.58  7.07   1  57\n",
       "4               73.88  36.14   6  374      12.77  6.82   1  64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nüìù TEXT CONTENT ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Text length statistics\n",
    "df['text_length'] = df['text'].apply(len)\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df['text_length'].describe())\n",
    "\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "# Character and word distribution by sentiment\n",
    "print(\"\\nüìä TEXT METRICS BY SENTIMENT\")\n",
    "text_by_sentiment = df.groupby('sentiment')[['text_length', 'word_count']].agg(['mean', 'std', 'min', 'max'])\n",
    "display(text_by_sentiment.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc327e0",
   "metadata": {},
   "source": [
    "##### **Length & Structure Patterns**\n",
    "Our analysis reveals **well-structured social media content** with meaningful text distributions:\n",
    "\n",
    "- **Average Text Length**: 74 characters - optimal for Twitter's 2009 character limit\n",
    "- **Word Density**: 13.2 words per tweet on average, indicating substantive content\n",
    "- **Content Range**: 6-374 characters span shows diverse expression lengths\n",
    "- **Distribution**: 75% of tweets contain fewer than 104 characters, confirming concise communication patterns\n",
    "\n",
    "\n",
    "##### **Sentiment-Specific Text Behavior**\n",
    "**Key Finding**: Negative tweets are slightly longer than positive tweets:\n",
    "- **Negative Sentiment (0)**: 74.3 characters, 13.6 words average\n",
    "- **Positive Sentiment (4)**: 73.9 characters, 12.8 words average\n",
    "\n",
    "**Business Implication**: Negative sentiment tends to involve more detailed explanations or complaints, while positive sentiment is more concise and direct. This pattern will inform our feature engineering approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bcecb",
   "metadata": {},
   "source": [
    "### Now, let us perform Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10659b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ TEMPORAL ANALYSIS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3109/29948063.py:5: FutureWarning: Parsed string \"Mon Apr 06 22:19:45 PDT 2009\" included an un-recognized timezone \"PDT\". Dropping unrecognized timezones is deprecated; in a future version this will raise. Instead pass the string without the timezone, then use .tz_localize to convert to a recognized timezone.\n",
      "  df['date_parsed'] = pd.to_datetime(df['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Range:\n",
      " Earliest: 2009-04-06 22:19:45\n",
      " Latest: 2009-06-25 10:28:31\n",
      " Span: 79 days 12:08:46 days\n",
      "\n",
      "Temporal Distribution of tweets:\n",
      "By Month:\n",
      " Month 4: 100,025 tweets\n",
      " Month 5: 576,367 tweets\n",
      " Month 6: 923,608 tweets\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìÖ TEMPORAL ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Convert date column to datetime\n",
    "df['date_parsed'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date_parsed'].dt.year\n",
    "df['month'] = df['date_parsed'].dt.month\n",
    "df['day'] = df['date_parsed'].dt.day\n",
    "df['hour'] = df['date_parsed'].dt.hour\n",
    "\n",
    "print(\"Date Range:\")\n",
    "print(f\" Earliest: {df['date_parsed'].min()}\")\n",
    "print(f\" Latest: {df['date_parsed'].max()}\")\n",
    "print(f\" Span: {df['date_parsed'].max() - df['date_parsed'].min()} days\")\n",
    "\n",
    "# Temporal distribution\n",
    "print(\"\\nTemporal Distribution of tweets:\")\n",
    "print(\"By Month:\")\n",
    "monthly_dist = df['month'].value_counts().sort_index()\n",
    "for month, count in monthly_dist.items():\n",
    "    print(f\" Month {month}: {count:,} tweets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa4d8a9",
   "metadata": {},
   "source": [
    "##### **Data Collection Timeline**\n",
    "The dataset spans **79 days** (April 6 - June 25, 2009), showing strong **temporal concentration**:\n",
    "\n",
    "**Monthly Progression**:\n",
    "- **April**: 100K tweets (6.3%) - Initial collection\n",
    "- **May**: 576K tweets (36.0%) - Ramping phase  \n",
    "- **June**: 924K tweets (57.7%) - Peak collection\n",
    "\n",
    "##### **Strategic Insights**\n",
    "- **Seasonal Representation**: Spring/early summer 2009 data\n",
    "- **Collection Strategy**: Accelerating data gathering approach\n",
    "- **Temporal Validity**: 2009 language patterns may require modern validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7145832e",
   "metadata": {},
   "source": [
    "### Let us now perform User and Flag (query) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa2aac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üë• USER AND FLAG ANALYSIS\n",
      "==================================================\n",
      "Unique Users: 659,775\n",
      "Average Tweets per User: 2.43\n",
      "\n",
      "Top 10 Active Users:\n",
      " lost_dog: 549 tweets\n",
      " webwoke: 345 tweets\n",
      " tweetpet: 310 tweets\n",
      " SallytheShizzle: 281 tweets\n",
      " VioletsCRUK: 279 tweets\n",
      " mcraddictal: 276 tweets\n",
      " tsarnick: 248 tweets\n",
      " what_bugs_u: 246 tweets\n",
      " Karen230683: 238 tweets\n",
      " DarkPiano: 236 tweets\n",
      "\n",
      "üè∑Ô∏è FLAG (SENTIMENT) ANALYSIS:\n",
      " Flag (Sentiment) 'NO_QUERY': 1,600,000 tweets (100.00%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüë• USER AND FLAG ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# User statistics\n",
    "unique_useres = df['user'].nunique()\n",
    "total_tweets = len(df)\n",
    "avg_tweets_per_user = total_tweets / unique_useres\n",
    "\n",
    "print(f\"Unique Users: {unique_useres:,}\")\n",
    "print(f\"Average Tweets per User: {avg_tweets_per_user:.2f}\")\n",
    "\n",
    "# Top active users\n",
    "print(\"\\nTop 10 Active Users:\")\n",
    "top_users = df['user'].value_counts().head(10)\n",
    "for user, count in top_users.items():\n",
    "    print(f\" {user}: {count} tweets\")\n",
    "\n",
    "# Flag analysis\n",
    "print(\"\\nüè∑Ô∏è FLAG (SENTIMENT) ANALYSIS:\")\n",
    "flag_dist = df['query'].value_counts()\n",
    "for flag, count in flag_dist.items():\n",
    "    print(f\" Flag (Sentiment) '{flag}': {count:,} tweets ({count/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43322874",
   "metadata": {},
   "source": [
    "#### üë• **User Engagement Patterns**\n",
    "\n",
    "##### **User Activity Distribution**\n",
    "**High User Diversity** with balanced engagement:\n",
    "- **Unique Users**: 659,775 individual accounts\n",
    "- **Average Activity**: 2.43 tweets per user\n",
    "- **Engagement Pattern**: Mostly casual users with few heavy contributors\n",
    "\n",
    "##### **Power User Analysis**\n",
    "**Top User Activity**: \"lost_dog\" with 549 tweets (0.034% of total)\n",
    "- **User Concentration**: Top 10 users contribute <0.5% of total tweets\n",
    "- **Implication**: Dataset represents **authentic diverse user base** rather than bot-dominated content\n",
    "\n",
    "#### **üéØ Sentiment Label Distribution**\n",
    "\n",
    "Finding: 100% of tweets have \"NO_QUERY\" flag, indicating the entire dataset was collected through general sentiment sampling rather than keyword-specific queries.\n",
    "\n",
    "##### **Strategic Advantage: This ensures:**\n",
    "**Exceptional Class Balance**:\n",
    "- **Topic Diversity**: No bias toward specific keywords or subjects\n",
    "- **Generalization Capability**: Models trained on this data will generalize across diverse domains\n",
    "- **Production Readiness**: Suitable for VelociSense Analytics' cross-industry applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80648bac",
   "metadata": {},
   "source": [
    "### Text Content Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f82162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç TEXT QUALITY ASSESSMENT\n",
      "==================================================\n",
      "Content Pattern Analysis:\n",
      " URLs: 76,584 tweets (4.79%)\n",
      " Mentions: 738,493 tweets (46.16%)\n",
      " Hashtags: 35,847 tweets (2.24%)\n",
      " Numbers: 366,487 tweets (22.91%)\n",
      " Punctuations: 1,410,819 tweets (88.18%)\n",
      "\n",
      "Very Short Tweets (<5 characters): 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüîç TEXT QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Special characters and patterns\n",
    "df['has_url'] = df['text'].str.contains(r'http[s]?://|www\\.', case = False, na=False)\n",
    "df['has_mention'] = df['text'].str.contains(r'@\\w+', na=False)\n",
    "df['has_hashtag'] = df['text'].str.contains(r'#\\w+', na=False)\n",
    "df['has_numbers'] = df['text'].str.contains(r'\\d+', na=False)\n",
    "df['has_punctuations'] = df['text'].str.contains(r'[!@#$%^&*(),.?\":{}|<>]', na=False)\n",
    "\n",
    "quality_metrics = {\n",
    "    'URLs': df['has_url'].sum(),\n",
    "    'Mentions': df['has_mention'].sum(),\n",
    "    'Hashtags': df['has_hashtag'].sum(),\n",
    "    'Numbers': df['has_numbers'].sum(),\n",
    "    'Punctuations': df['has_punctuations'].sum()\n",
    "}\n",
    "\n",
    "print(\"Content Pattern Analysis:\")\n",
    "for pattern, count in quality_metrics.items():\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\" {pattern}: {count:,} tweets ({percentage:.2f}%)\")\n",
    "\n",
    "# Empty or very short tweets\n",
    "short_tweets = (df['text_length'] < 5).sum()\n",
    "print(f\"\\nVery Short Tweets (<5 characters): {short_tweets:,} ({short_tweets/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71cfd5f",
   "metadata": {},
   "source": [
    "#### **üîç Content Quality Assessment**\n",
    "\n",
    "##### **Social Media Feature Prevalence**\n",
    "**Rich Social Media Context**:\n",
    "- **Mentions**: 46.16% of tweets contain user mentions (@username)\n",
    "- **Punctuation**: 88.18% contain varied punctuation (emotional expression)\n",
    "- **Numbers**: 22.91% include numerical content (dates, quantities)\n",
    "- **URLs**: 4.79% contain links (external references)\n",
    "- **Hashtags**: 2.24% use hashtag categorization\n",
    "\n",
    "##### **Content Authenticity Indicators**\n",
    "- **No Extremely Short Content**: Zero tweets under 5 characters\n",
    "- **Natural Language Patterns**: High punctuation usage indicates authentic expression\n",
    "- **Moderate URL Content**: Low spam-like link sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0258bc7",
   "metadata": {},
   "source": [
    "### Sample Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0148d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã SAMPLE TWEETS BY SENTIMENT\n",
      "==================================================\n",
      "\n",
      "Negative Sentiment (value=0) - Sample Tweets:\n",
      " 1. @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      " 2. is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
      " 3. @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
      " 4. my whole body feels itchy and like its on fire \n",
      " 5. @nationwideclass no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there. \n",
      "\n",
      "Positive Sentiment (value=4) - Sample Tweets:\n",
      " 1. I LOVE @Health4UandPets u guys r the best!! \n",
      " 2. im meeting up with one of my besties tonight! Cant wait!!  - GIRL TALK!!\n",
      " 3. @DaRealSunisaKim Thanks for the Twitter add, Sunisa! I got to meet you once at a HIN show here in the DC area and you were a sweetheart. \n",
      " 4. Being sick can be really cheap when it hurts too much to eat real food  Plus, your friends make you soup\n",
      " 5. @LovesBrooklyn2 he has that effect on everyone \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã SAMPLE TWEETS BY SENTIMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for sentiment_val in sorted(df['sentiment'].unique()):\n",
    "    label = \"Negative\" if sentiment_val == 0 else \"Neutral\" if sentiment_val == 2 else \"Positive\"\n",
    "    print(f\"\\n{label} Sentiment (value={sentiment_val}) - Sample Tweets:\")\n",
    "    samples = df[df['sentiment'] == sentiment_val]['text'].head(5)\n",
    "    for i, tweet in enumerate(samples, 1):\n",
    "        print(f\" {i}. {tweet}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5a493",
   "metadata": {},
   "source": [
    "##### **Content Authenticity Validation**\n",
    "**Sample Tweet Analysis Confirms**:\n",
    "- **Negative Samples**: Authentic complaints, frustrations, genuine emotions\n",
    "- **Positive Samples**: Genuine enthusiasm, social interactions, appreciations\n",
    "- **Language Patterns**: Natural social media communication style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b9414",
   "metadata": {},
   "source": [
    "###  Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed8c7e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà DATA QUALITY SUMMARY\n",
      " Total Records: 1600000\n",
      " Complete Records: 1600000\n",
      " Duplicate Records: 0\n",
      " Unique Users: 659775\n",
      " Data Completeness (%): 100.00%\n",
      " Text Uniqueness: 98.84%\n",
      "\n",
      "‚ö†Ô∏è POTENTIAL DATA ISSUES:\n",
      " .  18,534 duplicate tweet texts detected.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìà DATA QUALITY SUMMARY\")\n",
    "\n",
    "quality_report = {\n",
    "    'Total Records': len(df),\n",
    "    'Complete Records': len(df.dropna()),\n",
    "    'Duplicate Records': df.duplicated().sum(),\n",
    "    'Unique Users': df['user'].nunique(),\n",
    "    'Data Completeness (%)': f\"{(len(df.dropna())/len(df))*100:.2f}%\",\n",
    "    'Text Uniqueness': f\"{(df['text'].nunique()/len(df))*100:.2f}%\"\n",
    "}\n",
    "\n",
    "for metric, value in quality_report.items():\n",
    "    print(f\" {metric}: {value}\")\n",
    "\n",
    "# Identify potential issues\n",
    "print(\"\\n‚ö†Ô∏è POTENTIAL DATA ISSUES:\")\n",
    "issues = []\n",
    "\n",
    "if df.duplicated().sum() > 0:\n",
    "    issues.append(f\"- {df.duplicated().sum():,} duplicate records found.\")\n",
    "\n",
    "if (df['text'].str.len() <=1).sum() > 0:\n",
    "    issues.append(f\"- {(df['text'].str.len() <=1).sum():,} extremely short tweets\")\n",
    "\n",
    "if df['text'].nunique() < len(df):\n",
    "    duplicate_texts = len(df) - df['text'].nunique()\n",
    "    issues.append(f\" {duplicate_texts:,} duplicate tweet texts detected.\")\n",
    "\n",
    "if issues:\n",
    "    for issue in issues:\n",
    "        print(f\" . {issue}\")\n",
    "else:\n",
    "    print(\"‚úÖ No major data quality issues detected \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa1261",
   "metadata": {},
   "source": [
    "#### **‚ö†Ô∏è Data Considerations for Model Development**\n",
    "\n",
    "##### **Minor Quality Issues Identified**\n",
    "**Duplicate Text Content**: 18,534 instances (1.16%)\n",
    "- **Impact**: Minimal effect on 1.6M dataset\n",
    "- **Handling Strategy**: Consider deduplication during preprocessing comparison\n",
    "- **Business Context**: May represent common phrases or viral content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072ae99c",
   "metadata": {},
   "source": [
    "### EDA Conclusions & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b6fada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ EDA CONCLUSIONS\n",
      "====================\n",
      "Key Findings:\n",
      "‚úÖ Dataset loaded successfully with 'sentiment' as target variable\n",
      "‚úÖ 1,600,000 total records with balanced/imbalanced distribution identified\n",
      "‚úÖ Text length and quality patterns established\n",
      "‚úÖ Temporal and user activity patterns documented\n",
      "‚úÖ Data quality assessment completed\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ EDA CONCLUSIONS\")\n",
    "print(\"=\"*20)\n",
    "print(\"Key Findings:\")\n",
    "print(\"‚úÖ Dataset loaded successfully with 'sentiment' as target variable\")\n",
    "print(f\"‚úÖ {len(df):,} total records with balanced/imbalanced distribution identified\")\n",
    "print(\"‚úÖ Text length and quality patterns established\")\n",
    "print(\"‚úÖ Temporal and user activity patterns documented\")\n",
    "print(\"‚úÖ Data quality assessment completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7c261",
   "metadata": {},
   "source": [
    "## **üöÄ Strategic Implications for VelociSense Analytics**\n",
    "\n",
    "### **Model Development Advantages**\n",
    "1. **Balanced Training**: Perfect class distribution enables unbiased model training\n",
    "2. **Scale Robustness**: 1.6M samples support complex deep learning architectures\n",
    "3. **Feature Diversity**: Rich social media patterns for comprehensive feature engineering\n",
    "4. **Quality Foundation**: High data completeness ensures reliable model performance\n",
    "\n",
    "### **Business Application Readiness**\n",
    "1. **Real-world Patterns**: Authentic social media content mirrors production scenarios\n",
    "2. **Scalability Validation**: Large dataset validates enterprise deployment capabilities\n",
    "3. **Content Variety**: Diverse text patterns prepare models for various business contexts\n",
    "\n",
    "### **Technical Development Strategy**\n",
    "1. **Preprocessing Focus**: Address duplicate text content in systematic comparison\n",
    "2. **Feature Engineering**: Leverage social media patterns (mentions, URLs, punctuation)\n",
    "3. **Model Architecture**: Balanced dataset supports both traditional ML and deep learning\n",
    "4. **Evaluation Framework**: High-quality ground truth enables robust performance assessment\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"background-color: #000000ff; padding: 15px; border-radius: 8px; margin: 20px 0;\">\n",
    "<h3>üéØ <strong>Key Takeaway</strong></h3>\n",
    "<p><em>The Sentiment140 dataset provides an exceptional foundation for our systematic NLP comparison with perfect class balance, high data quality, and authentic social media content patterns that directly align with VelociSense Analytics' enterprise requirements.</em></p>\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "**Next Phase**: Proceed to systematic text preprocessing comparison, leveraging these insights to design targeted cleaning strategies that preserve the dataset's authentic social media characteristics while optimizing for sentiment classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46acef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìã NEXT STEPS:\n",
      "1. Proceed to systematic text preprocessing comparison\n",
      "2. Implement cleaning techniques based on EDA findings\n",
      "3. Prepare data for feature engineering evaluation\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüìã NEXT STEPS:\")\n",
    "print(\"1. Proceed to systematic text preprocessing comparison\")\n",
    "print(\"2. Implement cleaning techniques based on EDA findings\")  \n",
    "print(\"3. Prepare data for feature engineering evaluation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
