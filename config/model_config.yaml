# Model Architecture Configuration

# Vocabulary & Sequence
vocabulary:
  max_vocab_size: 20000
  max_sequence_length: 50

# Embeddings
embeddings:
  type: "word2vec"
  dimensions: 200
  trainable: false
  
  word2vec:
    vector_size: 200
    window: 5
    min_count: 2
    workers: 4
    sg: 1  # Skip-gram
    epochs: 10

# Model Architecture
architecture:
  lstm:
    units: 128
    return_sequences: true
    dropout: 0.0
  
  attention:
    enabled: true
  
  dense_layers:
    - units: 64
      activation: "relu"
      dropout: 0.5
  
  output:
    units: 1
    activation: "sigmoid"

# Training Configuration
training:
  batch_size: 128
  epochs: 20
  learning_rate: 0.001
  validation_split: 0.15
  test_split: 0.15
  random_seed: 42

# Callbacks
callbacks:
  early_stopping:
    monitor: "val_loss"
    patience: 5
    restore_best_weights: true
  
  reduce_lr:
    monitor: "val_loss"
    factor: 0.5
    patience: 3
    min_lr: 0.0000001
  
  model_checkpoint:
    monitor: "val_accuracy"
    save_best_only: true

# Prediction
prediction:
  threshold: 0.5
  batch_size: 32
